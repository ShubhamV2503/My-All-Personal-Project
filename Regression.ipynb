{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33728933-e0ba-41d9-9b8a-3783e4782aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa18ec-7c07-4b69-8d25-b6af825c982f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb141fe-9e77-4d85-8d7c-6d331857fe49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aed778-5a03-4ea8-86f7-290e19e2d4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "### Model Part --> Regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "### Regression\n",
    "models = [\n",
    "    ('Linear Regression', LinearRegression()),\n",
    "    \n",
    "    # Uncomment to add more models as needed\n",
    "    # ('Decision Tree Regressor', DecisionTreeRegressor()),\n",
    "    # ('Random Forest Regressor', RandomForestRegressor()),\n",
    "\n",
    "    ('Gradient Boosting Regressor', GradientBoostingRegressor()),\n",
    "    ('XGBoost Regressor', XGBRegressor()),\n",
    "    ('LightGBM Regressor', LGBMRegressor()),\n",
    "    ('CatBoost Regressor', CatBoostRegressor(verbose=0))  # Set verbose=0 to suppress output\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Iterate through models, creating a pipeline for each\n",
    "for name, model in models.items():\n",
    "    # Create the pipeline with preprocessing and model\n",
    "    pipeline = Pipeline(steps=[('regressor', model)])\n",
    "\n",
    "    # Fit the model\n",
    "    pipeline.fit(x_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions on the training set\n",
    "    y_train_pred = pipeline.predict(x_train_scaled)\n",
    "\n",
    "    # Calculate regression metrics for the training set\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "    # Print metrics for the training set\n",
    "    print(f'{name} Train MSE: {mse_train:.4f}')\n",
    "    print(f'{name} Train MAE: {mae_train:.4f}')\n",
    "    print(f'{name} Train R²: {r2_train:.4f}')\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_test_pred = pipeline.predict(x_test_scaled)\n",
    "\n",
    "    # Calculate regression metrics for the test set\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    # Print metrics for the test set\n",
    "    print(f'{name} Test MSE: {mse_test:.4f}')\n",
    "    print(f'{name} Test MAE: {mae_test:.4f}')\n",
    "    print(f'{name} Test R²: {r2_test:.4f}')\n",
    "\n",
    "    # Plot predictions vs actual values for the test set\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_test, y_test_pred, alpha=0.6, color='blue')\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'{name} Predictions vs Actual Values (Test Set)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Choose which model to optimize\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"GradientBoosting\", \"XGBoost\", \"LightGBM\", \"CatBoost\"])\n",
    "\n",
    "    if model_name == \"GradientBoosting\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "            \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0)\n",
    "        }\n",
    "        model = GradientBoostingRegressor(**params)\n",
    "\n",
    "    elif model_name == \"XGBoost\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "            \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0)\n",
    "        }\n",
    "        model = XGBRegressor(**params, random_state=42)\n",
    "\n",
    "    elif model_name == \"LightGBM\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", -1, 10),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "            \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0)\n",
    "        }\n",
    "        model = LGBMRegressor(**params, random_state=42)\n",
    "\n",
    "    elif model_name == \"CatBoost\":\n",
    "        params = {\n",
    "            \"iterations\": trial.suggest_int(\"iterations\", 100, 500),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 3, 10),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "        }\n",
    "        model = CatBoostRegressor(**params, random_state=42, verbose=0)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train_scaled, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(x_test_scaled)\n",
    "\n",
    "    # Evaluate using Mean Squared Error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse\n",
    "\n",
    "# Create the Optuna study and optimize\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best trial\n",
    "print(\"Best trial:\")\n",
    "print(f\"MSE: {study.best_value:.4f}\")\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "\n",
    "\n",
    "##########################################################################################################\n",
    "\n",
    "\n",
    "param_rest = {\n",
    "    'objective': 'RMSE',  # Changed to RMSE (Root Mean Squared Error) for regression\n",
    "    'colsample_bylevel': 0.09130944091779239,\n",
    "    'depth': 11,\n",
    "    'boosting_type': 'Plain',\n",
    "    'learning_rate': 0.08515526764930864,\n",
    "    'bootstrap_type': 'MVS',\n",
    "    'min_data_in_leaf': 100\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize the final model for regression\n",
    "final_model_new = CatBoostRegressor(logging_level='Silent', **param_rest, random_state=212, iterations=1000)\n",
    "\n",
    "# Fit the model on the training data\n",
    "final_model_new.fit(X_train_new, y_train_resample)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = final_model_new.predict(X_test_new)\n",
    "\n",
    "# Evaluate the model using regression metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")\n",
    "\n",
    "##########################################################################################################\n",
    "\n",
    "best_model = study.best_trial.user_attrs['best_model']\n",
    "\n",
    "feature_importance = best_model.get_feature_importance()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(feature_importance)), feature_importance, align=\"center\")\n",
    "plt.yticks(range(len(feature_importance)), [f\"Feature {i}\" for i in range(len(feature_importance))])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('CatBoost Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "pca = PCA(n_components=0.95)  \n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained by the selected components: {np.sum(pca.explained_variance_ratio_)}\")\n",
    "\n",
    "if X_pca.shape[1] == 2:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.title('PCA of Iris Dataset (95% Variance Retained)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Number of components retained: {X_pca.shape[1]}\")\n",
    "\n",
    "#####################################################################\n",
    "### Cross Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = LogisticRegression(max_iter=200)\n",
    "cv_scores = cross_val_score(model, X_scaled, y, cv=5)\n",
    "\n",
    "\n",
    "\n",
    "### Feture importance\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Display the feature importances\n",
    "features = data.feature_names\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(importance_df)\n",
    "\n",
    "\n",
    "##################################################################################################################################################\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Assuming X is your features and y is the target\n",
    "mi = mutual_info_classif(X, y)\n",
    "\n",
    "# Display the mutual information scores\n",
    "importance_df = pd.DataFrame({'Feature': X.columns, 'Mutual Information': mi})\n",
    "importance_df = importance_df.sort_values(by='Mutual Information', ascending=False)\n",
    "print(importance_df)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assuming X and y are your features and target variables\n",
    "model = LogisticRegression(max_iter=200)\n",
    "rfe = RFE(model, n_features_to_select=1)  # Rank all features\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Get ranking of features\n",
    "ranking = rfe.ranking_\n",
    "\n",
    "# Display the ranked features\n",
    "importance_df = pd.DataFrame({'Feature': X.columns, 'Ranking': ranking})\n",
    "importance_df = importance_df.sort_values(by='Ranking', ascending=True)\n",
    "print(importance_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf996a-351e-44ad-970a-0c191961c277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6b093-b9a3-4935-9ad8-a719cd300769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b79494e-f8a9-4918-9506-91169cb06a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6c83d-bba6-4570-b0ac-3bc6da7446fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f9c683-01e9-4a91-94ad-ec1cd884a8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d342283-38ae-465f-bcbf-70e9513d8546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8242acbc-5a53-49f5-8101-8ffd8e1383f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
