{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b9020-3ab0-47aa-b634-d8e5ad774178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1643c7-2e58-49a2-adfa-92566bf07d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52abb8b-b1ea-486c-ba43-f5f532582a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "## Steps 1 --->\n",
    "\n",
    "df.duplicated().sum()\n",
    "\n",
    "### Step 2  EDA ---->\n",
    "\n",
    "\n",
    "\n",
    "### Step 3\n",
    "### Outlier Handling ------>\n",
    "\n",
    "def detect_outliers_in_columns(df):\n",
    "\n",
    "    outlier_columns = []\n",
    "    numerical_cols = df.select_dtypes(include=['int64']).columns\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "\n",
    "        if ((df[col] < lower_bound) | (df[col] > upper_bound)).any():\n",
    "            outlier_columns.append(col)\n",
    "\n",
    "    return outlier_columns\n",
    "\n",
    "\n",
    "\n",
    "def count_of_outliers(df, outlier_columns): \n",
    "    outlier_counts = {}\n",
    "    for col in outlier_columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outlier_count = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "        outlier_counts[col] = outlier_count\n",
    "    return outlier_counts\n",
    "\n",
    "\n",
    "def cap_outliers(df, outlier_columns):\n",
    "    for col in outlier_columns:\n",
    "        if col != 'Response':\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "\n",
    "            df.loc[df[col] < lower_bound, col] = lower_bound\n",
    "            df.loc[df[col] > upper_bound, col] = upper_bound\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "outlier_columns = detect_outliers_in_columns(df)\n",
    "print(\"Columns with outliers:\", outlier_columns)\n",
    "outlier_counts = count_of_outliers(df, outlier_columns)\n",
    "print(\"Outlier counts per column:\", outlier_counts)\n",
    "df_capped = cap_outliers(df, outlier_columns)\n",
    "outlier_counts = count_of_outliers(df, outlier_columns)\n",
    "print(\"Outlier counts per column:\", outlier_counts)\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "df = df_capped.copy()\n",
    "del df_capped\n",
    "\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "# train['Job Satisfaction'] = train['Job Satisfaction'].fillna(train['Job Satisfaction'].median())\n",
    "# train.isnull().sum()/train.shape[0]*100\n",
    "# train['Job Satisfaction'] = train['Job Satisfaction'].fillna(train['Job Satisfaction'].mode()[0])\n",
    "\n",
    "\n",
    "\n",
    "# For Numerical\n",
    "# # skew\n",
    "\n",
    "# -0.5 to 0.5  -- mean\n",
    "# -1 and -0.5 or 0.5 and 1 --> median\n",
    "# less than -1 and greater than 1  --> median\n",
    "\n",
    "# For categorical\n",
    "## Nominal data --> mode\n",
    "## Ordinal data --> median\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "### Encoding\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "lab = LabelEncoder()\n",
    "df['Gender' ]  = lab.fit_transform(df['Gender'])\n",
    "df['Vehicle_Damage' ]  = lab.fit_transform(df['Vehicle_Damage'])\n",
    "\n",
    "vehical_age = ['< 1 Year', '1-2 Year', '> 2 Years']  \n",
    "ordinal_encoder = OrdinalEncoder(categories=[vehical_age])\n",
    "df['Vehicle_Age'] = ordinal_encoder.fit_transform(df[['Vehicle_Age']])\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=['Vehicle_Age'], drop_first=True)  \n",
    "bool = df_encoded.select_dtypes(include=['bool']).columns\n",
    "df_encoded[bool] = df_encoded[bool].astype(int)\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "## Independent and dependence data\n",
    "\n",
    "y = df_encoded['Response']\n",
    "x = df_encoded.drop(columns = ['Response'])\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train_fit = scaler.fit_transform(x)\n",
    "x_train_scaled = pd.DataFrame(x_train_fit, columns=x.columns)\n",
    "\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "### Train Test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=9,stratify=y)\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "### Balancing datasets\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Applying SMOTE for oversampling\n",
    "smote = SMOTE(random_state=42)\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "### Model Part --> Classification\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "### Classification\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "\n",
    "    # ('Support Vector Machine', SVC()),\n",
    "\n",
    "    # ('Decision Tree', DecisionTreeClassifier()),\n",
    "    # ('Random Forest', RandomForestClassifier()),\n",
    "\n",
    "\n",
    "    ('Gradient Boosting', GradientBoostingClassifier()),\n",
    "    ('XGBoost', XGBClassifier()),\n",
    "    ('LightGBM', LGBMClassifier()),\n",
    "    ('CatBoost', CatBoostClassifier()),\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc , accuracy_score\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "## CLassification\n",
    "\n",
    "\n",
    "for name, model in models:\n",
    "    pipeline = Pipeline(steps=[('classifier', model)])\n",
    "    pipeline.fit(x_train_scaled, y_train)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_train_pred_prob = pipeline.predict_proba(x_train_scaled)[:, 1]\n",
    "        fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_pred_prob)\n",
    "        roc_auc_train = auc(fpr_train, tpr_train)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr_train, tpr_train, color='blue', lw=2, label=f'Train ROC curve (AUC = {roc_auc_train:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Diagonal line\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.0])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'{name} Train ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        print(f'{name} Train AUC: {roc_auc_train:.2f}')\n",
    "        y_test_pred_prob = pipeline.predict_proba(x_test_scaled)[:, 1]\n",
    "        fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_pred_prob)\n",
    "        roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr_test, tpr_test, color='green', lw=2, label=f'Test ROC curve (AUC = {roc_auc_test:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Diagonal line\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.0])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'{name} Test ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        print(f'{name} Test AUC: {roc_auc_test:.2f}')\n",
    "\n",
    "    else:\n",
    "        print(f'{name} does not support predict_proba. Skipping ROC curve plotting.')\n",
    "        y_train_pred = pipeline.predict(x_train_scaled)\n",
    "        print(f'{name} Train Accuracy: {accuracy_score(y_train, y_train_pred):.2f}')\n",
    "\n",
    "        y_test_pred = pipeline.predict(x_test_scaled)\n",
    "        print(f'{name} Test Accuracy: {accuracy_score(y_test, y_test_pred):.2f}')\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,ExtraTreesClassifier,AdaBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.feature_selection import RFECV # Recursive Feature Elimination with Cross-Validation.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier,Pool\n",
    "from sklearn.model_selection import train_test_split,KFold,cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from category_encoders import CatBoostEncoder\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,roc_curve,recall_score,precision_score,f1_score,balanced_accuracy_score,roc_auc_score,ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "\n",
    "## Optuna code\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Choose which model to optimize\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"GradientBoosting\", \"XGBoost\", \"LightGBM\", \"CatBoost\"])\n",
    "\n",
    "    if model_name == \"GradientBoosting\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "            \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0)\n",
    "        }\n",
    "        model = GradientBoostingRegressor(**params)\n",
    "\n",
    "    elif model_name == \"XGBoost\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "            \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0)\n",
    "        }\n",
    "        model = XGBRegressor(**params, random_state=42)\n",
    "\n",
    "    elif model_name == \"LightGBM\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", -1, 10),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "            \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0)\n",
    "        }\n",
    "        model = LGBMRegressor(**params, random_state=42)\n",
    "\n",
    "    elif model_name == \"CatBoost\":\n",
    "        params = {\n",
    "            \"iterations\": trial.suggest_int(\"iterations\", 100, 500),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 3, 10),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "        }\n",
    "        model = CatBoostRegressor(**params, random_state=42, verbose=0)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train_scaled, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(x_test_scaled)\n",
    "\n",
    "    # Evaluate using Mean Squared Error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse\n",
    "\n",
    "# Create the Optuna study and optimize\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best trial\n",
    "print(\"Best trial:\")\n",
    "print(f\"MSE: {study.best_value:.4f}\")\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "### Feature Importance\n",
    "\n",
    "best_model = study.best_trial.user_attrs['best_model']\n",
    "\n",
    "feature_importance = best_model.get_feature_importance()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(feature_importance)), feature_importance, align=\"center\")\n",
    "plt.yticks(range(len(feature_importance)), [f\"Feature {i}\" for i in range(len(feature_importance))])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('CatBoost Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#######################\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "\n",
    "################################################\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "pca = PCA(n_components=0.95)  \n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained by the selected components: {np.sum(pca.explained_variance_ratio_)}\")\n",
    "if X_pca.shape[1] == 2:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.title('PCA of Iris Dataset (95% Variance Retained)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Number of components retained: {X_pca.shape[1]}\")\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "### Cross Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = LogisticRegression(max_iter=200)\n",
    "cv_scores = cross_val_score(model, X_scaled, y, cv=5)\n",
    "\n",
    "#####################################################################\n",
    "### Feture importance\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Display the feature importances\n",
    "features = data.feature_names\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(importance_df)\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################################################################\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Assuming X is your features and y is the target\n",
    "mi = mutual_info_classif(X, y)\n",
    "\n",
    "# Display the mutual information scores\n",
    "importance_df = pd.DataFrame({'Feature': X.columns, 'Mutual Information': mi})\n",
    "importance_df = importance_df.sort_values(by='Mutual Information', ascending=False)\n",
    "print(importance_df)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assuming X and y are your features and target variables\n",
    "model = LogisticRegression(max_iter=200)\n",
    "rfe = RFE(model, n_features_to_select=1)  # Rank all features\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Get ranking of features\n",
    "ranking = rfe.ranking_\n",
    "\n",
    "# Display the ranked features\n",
    "importance_df = pd.DataFrame({'Feature': X.columns, 'Ranking': ranking})\n",
    "importance_df = importance_df.sort_values(by='Ranking', ascending=True)\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b2302-ca3e-460c-8378-19d02b9a31ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e05a6-db10-4b73-a6fc-fd66766ad956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892f752-be97-4601-bd0b-49424d07e55b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fcab55-10cf-406e-9a55-656fcf68c071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759acde7-27f9-433a-93f7-1d67349db9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d66344-bf31-48bf-b228-903900abcc21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
